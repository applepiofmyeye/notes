# Probability - recap

- A, B independent if P[A, B] = P(A) * P(B) -- If event A doesnt affect the probability of B
- E[X+Y] = E[X] + E[Y]

- if X, Y independent
  - E[XY] = E[X]E[Y]
  - Var(X + Y) = Var(X) + Var(Y)

- (1 - (1/n))^n ≈ 1/e

- Markov's inequality: for any non-neg random var. X and any alpha > 1, 
  - P(X > å(E[X])) <= 1/å
- Chebyshev's inequality

- sqrt(Variance) = SD (standard deviation)
  - which is how much the X deviates from E(X)

- Cov(Xi, Xj) = E[XiXj] - E[Xi]E[Xj]

- ?? pairwise independent? what does that mean


- Chernoff Bound
  - when variables are fully independent

